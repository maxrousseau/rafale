run:
  name: "bert_tiny-imdb" # name of your experiment, used for checkpointing
  seed: 42
  n_epochs: 1
  max_lr: 6e-04
  warmup_pct: 0.01
  schedule: "cosine-warmup" # linear, linear-warmup, cosine, cosine-warmup
  optimizer: "AdamW"
  eval_interval: "50ba"
  clip_type: "norm"
  clip_value: 1.0
  device_bs: "auto"
  save_interval: "200ba"

model:
  config: "berttiny" # config key
  type: "encoder"
  mode: "cls"
  n_classes: 2 # not used unless multiclass classification
  use_pretrained: True

data:
  pipeline: "imdb_bert" # the preprocessing/tokenization pipeline
  config:
    name: "imdb"
    num_processes: 8
    tokenizer_name: "bert"
    input_id_key: "input_ids"
    train_batch_size: 1024
    eval_batch_size: 16
    shuffle_train: True
    dataset_path: "stanfordnlp/imdb" # can be local or path to hf repo
    tokenizer_path: "google-bert/bert-base-uncased" # ^same
    max_sequence_length: 128 # might not be sufficient...
    pad_token_id: -100
    pad_inputs: True
    is_prepared: False

logging: # @TODO :: not implemented
  use_wandb: True
  use_file: False
  eval_interval: "10ba"
  log_dir: "./run_logs"
  checkpoint_dir: "./checkpoints"
